# Databricks Unity Catalog Setup with Azure Storage Account

## Overview

This guide provides step-by-step instructions to connect Databricks Unity Catalog with Azure Data Lake Storage Gen2 for centralized data governance and secure access control across your data lakehouse architecture.

## Prerequisites

- Azure subscription with Contributor permissions
- Databricks Premium tier workspace (Unity Catalog is not supported on Standard tier)
- Azure Data Lake Storage Gen2 account with containers (bronze, silver, gold)
- Access to Databricks Account Console

## Step 1: Prepare Azure Resources

### 1.1 Verify Databricks Workspace Tier
1. Navigate to your Databricks workspace in Azure Portal
2. Check that the pricing tier is **Premium**
3. If using Standard tier, upgrade to Premium to enable Unity Catalog features

### 1.2 Confirm Storage Account Setup
1. Ensure your Azure Data Lake Storage Gen2 account is created
2. Verify that hierarchical namespace is enabled
3. Create containers for each layer: `bronze`, `silver`, `gold`
4. Note down the storage account name for later configuration

## Step 2: Configure Authentication with Managed Identity

### 2.1 Set Up Access Connector for Databricks
1. In Azure Portal, search for "Access Connector for Azure Databricks"
2. Create a new Access Connector resource
3. Select the same resource group and region as your Databricks workspace
4. This will create a system-assigned managed identity for secure authentication

### 2.2 Grant Storage Permissions
1. Navigate to your Azure Data Lake Storage Gen2 account
2. Go to **Access Control (IAM)**
3. Click **Add role assignment**
4. Assign **Storage Blob Data Contributor** role to the Databricks Access Connector managed identity
5. This enables read/write access to your storage containers

## Step 3: Create Unity Catalog Metastore

### 3.1 Access Databricks Account Console
1. Open a web browser and navigate to `https://accounts.azuredatabricks.net`
2. Sign in using your Azure Active Directory credentials
3. Ensure you have account admin permissions

### 3.2 Create the Metastore
1. In the Account Console, click on **Data** in the left navigation
2. Select **Create Metastore**
3. Provide a meaningful name for your metastore
4. Choose the Azure region that matches your Databricks workspace
5. Configure the metastore to use your Azure Data Lake Storage account
6. Complete the creation process

## Step 4: Link Workspace to Metastore

### 4.1 Assign Metastore to Workspace
1. In the Databricks Account Console, navigate to **Workspaces**
2. Find and select your Databricks workspace
3. Click on **Settings** or **Configuration**
4. In the **Unity Catalog** section, assign the metastore you created
5. Confirm the assignment and wait for the configuration to complete

### 4.2 Verify Assignment
1. Navigate to your Databricks workspace
2. Check that Unity Catalog is now available in the left navigation
3. Verify that you can see the **Data** section with catalog functionality

## Step 5: Configure Storage Credentials and External Locations

### 5.1 Create Storage Credential
1. In your Databricks workspace, open a SQL Editor or Notebook
2. Create a storage credential that uses the Azure managed identity
3. This credential will handle authentication to your storage account
4. Give it a descriptive name for easy identification

### 5.2 Set Up External Locations
1. Create external locations for each data layer (bronze, silver, gold)
2. Each external location points to the corresponding container in your storage account
3. Associate each external location with the storage credential created above
4. Use the ABFSS protocol format for the storage URLs

### 5.3 Test External Location Access
1. Use Databricks file system commands to verify access to each container
2. List the contents of each storage container to confirm connectivity
3. Troubleshoot any permission issues if access fails

## Step 6: Create Catalogs and Schemas

### 6.1 Create Development Catalog
1. In Databricks SQL Editor, create a new catalog for your project
2. Use a descriptive name that reflects your project purpose
3. This catalog will contain all schemas for your data layers

### 6.2 Create Schemas for Each Data Layer
1. Create a **bronze** schema with managed location pointing to bronze container
2. Create a **silver** schema with managed location pointing to silver container  
3. Create a **gold** schema with managed location pointing to gold container
4. Each schema represents a layer in your medallion architecture

### 6.3 Verify Schema Configuration
1. List all schemas in your catalog to confirm creation
2. Check the detailed properties of each schema
3. Verify that managed locations point to correct storage paths

## Step 7: Test Data Access and Operations

### 7.1 Test Table Creation
1. Create a test table in the bronze schema using external data
2. Create a test table in the silver schema using processed data
3. Verify that tables are accessible and queryable

### 7.2 Validate Cross-Layer Access
1. Test reading data from bronze layer in silver layer processing
2. Verify that gold layer can access silver layer data
3. Confirm that the medallion architecture data flow works correctly

### 7.3 Test Security and Permissions
1. Verify that users have appropriate access to different layers
2. Test that unauthorized access is properly blocked
3. Confirm that audit logging is working for data access

## Step 8: Configure Data Governance

### 8.1 Set Up Access Controls
1. Define user groups for different roles (data engineers, analysts, consumers)
2. Grant appropriate permissions to each group for different schemas
3. Implement column-level security if required for sensitive data

### 8.2 Enable Audit Logging
1. Configure audit logging to track data access and modifications
2. Set up monitoring for unusual access patterns
3. Establish regular review processes for access logs

### 8.3 Implement Data Lineage
1. Enable automatic lineage tracking for your data pipelines
2. Document data transformation processes
3. Set up lineage visualization for stakeholders

## Verification Checklist

Before proceeding with your data pipeline implementation, verify:

- [ ] Unity Catalog is enabled and accessible in your workspace
- [ ] All external locations are created and accessible
- [ ] Storage credentials are properly configured
- [ ] Catalogs and schemas are created with correct managed locations
- [ ] Test tables can be created and queried in each layer
- [ ] Cross-layer data access works correctly
- [ ] Security permissions are properly configured
- [ ] Audit logging is enabled and functioning

## Best Practices

### Security
- Always use managed identity for authentication
- Apply principle of least privilege for user access
- Regularly review and audit permissions
- Enable network security features if required

### Organization
- Use consistent naming conventions for catalogs and schemas
- Document your data governance policies
- Maintain clear separation between development and production environments
- Implement proper data classification and tagging

### Monitoring
- Set up alerts for failed authentication attempts
- Monitor storage costs and usage patterns
- Track data lineage and transformation processes
- Regularly backup metastore configurations

## Troubleshooting Common Issues

### Access Denied Errors
- Verify managed identity has correct storage permissions
- Check that external locations are properly configured
- Ensure workspace is properly linked to metastore

### Metastore Creation Failures
- Confirm you have account admin permissions
- Verify storage account accessibility from Databricks region
- Check that all required Azure resources are in same region

### Schema Creation Issues
- Verify external locations exist and are accessible
- Check storage credential configuration
- Ensure container names match configuration

## Next Steps

After successful Unity Catalog setup:
1. Begin implementing your bronze layer data ingestion processes  
2. Set up silver layer data transformation pipelines
3. Create gold layer views and tables for business consumption
4. Implement monitoring and alerting for your data pipelines
5. Train your team on Unity Catalog governance features
